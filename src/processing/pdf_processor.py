import PyPDF2
import os
import tiktoken
from typing import List, Dict, Tuple
import numpy as np
from config.config_manager import config_manager
from datetime import datetime
import hashlib

# Importar OpenAI de forma compatible
try:
    import openai
    # Verificar si es la nueva versi√≥n (1.0.0+)
    if hasattr(openai, 'OpenAI'):
        OPENAI_NEW = True
    else:
        OPENAI_NEW = False
except ImportError:
    OPENAI_NEW = False

class PDFProcessor:
    def __init__(self):
        self.api_key = config_manager.get_api_key()
        
        # Inicializar cliente OpenAI de forma compatible
        if OPENAI_NEW and self.api_key:
            try:
                self.openai_client = openai.OpenAI(api_key=self.api_key)
                self.openai_available = True
            except Exception as e:
                print(f"‚ö†Ô∏è Error inicializando cliente OpenAI: {e}")
                self.openai_available = False
        else:
            self.openai_available = False
            
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # Configuraciones desde la secci√≥n espec√≠fica
        self.tamano_fragmento = config_manager.get_tamano_fragmento()
        self.solapamiento_fragmento = config_manager.get_solapamiento_fragmento()
        self.modelo_embeddings = config_manager.get_modelo_embeddings()
        self.batch_size = config_manager.get_batch_size_embeddings()
    
    def contar_tokens(self, texto: str) -> int:
        """Contar tokens en un texto"""
        return len(self.encoding.encode(texto))
    
    def extraer_texto_pdf(self, pdf_path: str) -> Tuple[List[Dict], int]:
        """Extraer texto de un PDF y dividirlo en fragmentos optimizados"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_paginas = len(pdf_reader.pages)
                fragmentos = []
                
                max_fragmentos_por_pagina = config_manager.get(
                    "biblioteca_ia", "procesamiento.max_fragmentos_por_pagina", 10
                )
                min_longitud = config_manager.get(
                    "biblioteca_ia", "procesamiento.min_longitud_fragmento", 50
                )
                
                print(f"üìñ Procesando PDF: {os.path.basename(pdf_path)}")
                print(f"üìÑ Total de p√°ginas: {total_paginas}")
                
                for page_num in range(total_paginas):
                    page = pdf_reader.pages[page_num]
                    texto = page.extract_text()
                    
                    if texto and texto.strip():
                        # Limpiar y normalizar texto
                        texto = self._limpiar_texto(texto)
                        
                        # Dividir en fragmentos inteligentes
                        fragmentos_pagina = self._dividir_texto_en_fragmentos(
                            texto, page_num + 1, min_longitud
                        )
                        
                        # Limitar fragmentos por p√°gina si es necesario
                        if len(fragmentos_pagina) > max_fragmentos_por_pagina:
                            fragmentos_pagina = fragmentos_pagina[:max_fragmentos_por_pagina]
                        
                        fragmentos.extend(fragmentos_pagina)
                
                print(f"‚úÖ Extra√≠dos {len(fragmentos)} fragmentos de {total_paginas} p√°ginas")
                return fragmentos, total_paginas
                
        except Exception as e:
            print(f"‚ùå Error procesando PDF {pdf_path}: {e}")
            return [], 0
    
    def _limpiar_texto(self, texto: str) -> str:
        """Limpiar y normalizar texto"""
        # Eliminar m√∫ltiples espacios y saltos de l√≠nea
        lineas = [linea.strip() for linea in texto.split('\n') if linea.strip()]
        texto = ' '.join(lineas)
        
        # Limpiar caracteres extra√±os pero mantener puntuaci√≥n
        texto = ' '.join(texto.split())  # Normalizar espacios
        return texto
    
    def _dividir_texto_en_fragmentos(self, texto: str, pagina: int, min_longitud: int) -> List[Dict]:
        """Dividir texto en fragmentos inteligentes"""
        fragmentos = []
        
        # Intentar dividir por p√°rrafos naturales primero
        parrafos = [p.strip() for p in texto.split('\n\n') if p.strip() and len(p.strip()) >= min_longitud]
        
        for parrafo in parrafos:
            tokens = self.contar_tokens(parrafo)
            if tokens <= self.tamano_fragmento:
                fragmentos.append({
                    'contenido': parrafo,
                    'pagina': pagina,
                    'token_count': tokens
                })
            else:
                # Si el p√°rrafo es muy largo, dividir por oraciones
                oraciones = [o.strip() + '.' for o in parrafo.split('.') if o.strip()]
                fragmento_actual = ""
                
                for oracion in oraciones:
                    tokens_oracion = self.contar_tokens(oracion)
                    tokens_actual = self.contar_tokens(fragmento_actual)
                    
                    if tokens_actual + tokens_oracion <= self.tamano_fragmento:
                        if fragmento_actual:
                            fragmento_actual += " " + oracion
                        else:
                            fragmento_actual = oracion
                    else:
                        if fragmento_actual and self.contar_tokens(fragmento_actual) >= min_longitud:
                            fragmentos.append({
                                'contenido': fragmento_actual.strip(),
                                'pagina': pagina,
                                'token_count': self.contar_tokens(fragmento_actual)
                            })
                        fragmento_actual = oracion
                
                # Agregar el √∫ltimo fragmento
                if fragmento_actual and self.contar_tokens(fragmento_actual) >= min_longitud:
                    fragmentos.append({
                        'contenido': fragmento_actual.strip(),
                        'pagina': pagina,
                        'token_count': self.contar_tokens(fragmento_actual)
                    })
        
        return fragmentos
    
    def generar_embeddings_lote(self, textos: List[str]) -> List[List[float]]:
        """Generar embeddings en lote para mejor performance"""
        try:
            if not textos or not self.openai_available:
                return []
                
            print(f"üßÆ Generando embeddings para {len(textos)} textos...")
            
            if OPENAI_NEW:
                response = self.openai_client.embeddings.create(
                    model=self.modelo_embeddings,
                    input=textos
                )
                embeddings = [item.embedding for item in response.data]
            else:
                # Fallback para versiones antiguas (no recomendado)
                import openai as openai_old
                openai_old.api_key = self.api_key
                response = openai_old.Embedding.create(
                    model=self.modelo_embeddings,
                    input=textos
                )
                embeddings = [item['embedding'] for item in response['data']]
            
            print(f"‚úÖ Embeddings generados exitosamente")
            return embeddings
            
        except Exception as e:
            print(f"‚ùå Error generando embeddings en lote: {e}")
            return []
    
    def generar_embedding(self, texto: str) -> List[float]:
        """Generar embedding vectorial para un texto individual"""
        try:
            if not self.openai_available:
                return []
                
            if OPENAI_NEW:
                response = self.openai_client.embeddings.create(
                    model=self.modelo_embeddings,
                    input=texto
                )
                return response.data[0].embedding
            else:
                # Fallback para versiones antiguas
                import openai as openai_old
                openai_old.api_key = self.api_key
                response = openai_old.Embedding.create(
                    model=self.modelo_embeddings,
                    input=texto
                )
                return response['data'][0]['embedding']
        except Exception as e:
            print(f"‚ùå Error generando embedding: {e}")
            return []
        
    def extraer_metadatos_pdf(self, pdf_path: str) -> Dict:
        """Extraer metadatos avanzados del PDF"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Metadatos b√°sicos del PDF
                info = pdf_reader.metadata
                
                # Calcular hash del archivo (para evitar duplicados)
                file_hash = self._calcular_hash_archivo(pdf_path)
                
                # Estad√≠sticas del archivo
                file_stats = os.stat(pdf_path)
                
                metadatos = {
                    # Metadatos del archivo
                    'tamano_archivo_mb': round(file_stats.st_size / (1024 * 1024), 2),
                    'fecha_creacion_archivo': datetime.fromtimestamp(file_stats.st_ctime).isoformat(),
                    'fecha_modificacion_archivo': datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    'hash_archivo': file_hash,
                    
                    # Metadatos del PDF (si est√°n disponibles)
                    'titulo_pdf': info.get('/Title', ''),
                    'autor_pdf': info.get('/Author', ''),
                    'asunto_pdf': info.get('/Subject', ''),
                    'palabras_clave_pdf': info.get('/Keywords', ''),
                    'creador_pdf': info.get('/Creator', ''),
                    'productor_pdf': info.get('/Producer', ''),
                    'fecha_creacion_pdf': info.get('/CreationDate', ''),
                    'fecha_modificacion_pdf': info.get('/ModDate', ''),
                    
                    # Metadatos extra√≠dos del contenido
                    'idioma_detectado': 'espa√±ol',  # Podr√≠as detectar esto
                    'tipo_documento': 'libro',      # Podr√≠as clasificar
                    'calidad_extraccion': 'alta'    # Basado en √©xito del procesamiento
                }
                
                # Limpiar metadatos vac√≠os
                metadatos = {k: v for k, v in metadatos.items() if v}
                
                return metadatos
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error extrayendo metadatos: {e}")
            return {
                'tamano_archivo_mb': round(os.path.getsize(pdf_path) / (1024 * 1024), 2),
                'hash_archivo': self._calcular_hash_archivo(pdf_path),
                'error_metadatos': str(e)
            }
    
    def _calcular_hash_archivo(self, file_path: str) -> str:
        """Calcular hash MD5 del archivo para detectar duplicados"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def extraer_metadatos_del_contenido(self, texto_completo: str) -> Dict:
        """Intentar extraer metadatos analizando el contenido del texto"""
        # Dividir en l√≠neas para an√°lisis
        lineas = texto_completo.split('\n')
        
        metadatos = {
            'total_palabras': len(texto_completo.split()),
            'total_lineas': len(lineas),
            'total_caracteres': len(texto_completo),
        }
        
        # Intentar detectar autor en primeras l√≠neas
        primeras_lineas = lineas[:10]
        for i, linea in enumerate(primeras_lineas):
            linea_limpia = linea.strip()
            if len(linea_limpia) > 3 and len(linea_limpia) < 100:
                # Patrones comunes de nombres de autor
                if any(palabra in linea_limpia.lower() for palabra in ['por', 'author', 'autor', 'written by']):
                    metadatos['autor_detectado'] = linea_limpia
                    break
        
        # Intentar detectar ISBN (patr√≥n espec√≠fico)
        import re
        isbn_pattern = r'\b(?:ISBN(?:: ?| ))?((?:97[89])?\d{9}[\dxX])\b'
        isbn_match = re.search(isbn_pattern, texto_completo)
        if isbn_match:
            metadatos['isbn_detectado'] = isbn_match.group(1)
        
        return metadatos